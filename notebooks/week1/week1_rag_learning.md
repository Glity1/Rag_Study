# 🎯 1주차: 프로젝트 기획 및 RAG의 이해

## 학습 목표
RAG 아키텍처의 전체 그림을 이해하고, 프로젝트에 필요한 핵심 구성 요소(벡터 DB, 임베딩, LLM)의 개념을 학습합니다.

---

## 📚 1. RAG(Retrieval-Augmented Generation)란?

### 🤔 왜 LLM만으로는 부족한가?

LLM(대규모 언어 모델)은 매우 강력하지만, 다음과 같은 한계가 있습니다:

1. **지식의 시간적 한계**
   - LLM은 특정 시점까지의 데이터로 학습됨
   - 최신 정보나 실시간 데이터에 대해 답변할 수 없음
   - 예: "2024년 11월 현재 환율은?" → 답변 불가

2. **도메인 특화 지식 부족**
   - 회사 내부 문서, 특정 기술 매뉴얼 등은 학습 데이터에 없음
   - 예: "우리 회사의 휴가 정책은?" → 답변 불가

3. **환각(Hallucination) 문제**
   - 모르는 내용에 대해 그럴듯한 거짓말을 생성할 수 있음
   - 확신을 가지고 잘못된 정보를 제공하는 위험

4. **검증 가능성 부족**
   - 답변의 근거를 제시하지 못함
   - 출처를 확인할 수 없어 신뢰성 검증이 어려움

### 💡 RAG가 필요한 이유

RAG는 이러한 문제를 해결하기 위해 등장했습니다:

✅ **외부 지식 베이스 활용**: 최신 문서, 특정 도메인 지식 접근 가능  
✅ **근거 기반 답변**: 검색된 문서를 근거로 답변 생성  
✅ **환각 감소**: 문서에 없는 내용은 "모른다"고 답변 가능  
✅ **검증 가능**: 어떤 문서를 참고했는지 추적 가능  

### 🔄 RAG 작동 원리 (3단계)

```
┌─────────────────────────────────────────────────────────┐
│                    RAG 시스템 전체 구조                    │
└─────────────────────────────────────────────────────────┘

[1단계: Indexing (색인 - 사전 작업)]
┌──────────────┐      ┌──────────────┐      ┌──────────────┐
│   PDF 문서    │ ───> │  청킹(분할)   │ ───> │  임베딩 변환  │
│ (원본 자료)   │      │ (작은 조각)   │      │  (벡터화)     │
└──────────────┘      └──────────────┘      └──────────────┘
                                                      │
                                                      ▼
                                            ┌──────────────┐
                                            │   벡터 DB    │
                                            │  (저장소)    │
                                            └──────────────┘

[2단계: Retrieval (검색 - 실시간)]
┌──────────────┐      ┌──────────────┐      ┌──────────────┐
│ 사용자 질문   │ ───> │  질문 임베딩  │ ───> │  유사도 검색  │
│ "휴가는 며칠?"│      │  (벡터화)     │      │  (벡터 DB)   │
└──────────────┘      └──────────────┘      └──────────────┘
                                                      │
                                                      ▼
                                            ┌──────────────┐
                                            │ 관련 문서 조각│
                                            │  (Top K)     │
                                            └──────────────┘

[3단계: Generation (생성 - 실시간)]
┌──────────────┐      ┌──────────────┐      ┌──────────────┐
│검색된 문서 +  │ ───> │     LLM      │ ───> │  최종 답변   │
│ 사용자 질문   │      │  (답변 생성)  │      │  (근거 포함) │
└──────────────┘      └──────────────┘      └──────────────┘
```

#### 1️⃣ Indexing (색인) - 사전 준비 단계
- **목적**: 문서를 검색 가능한 형태로 준비
- **과정**:
  1. 문서를 작은 청크(조각)로 분할
  2. 각 청크를 임베딩 모델로 벡터(숫자 배열)로 변환
  3. 벡터를 벡터 DB에 저장
- **비유**: 도서관에서 책을 분류하고 목록을 만드는 과정

#### 2️⃣ Retrieval (검색) - 질문이 들어왔을 때
- **목적**: 질문과 가장 관련 있는 문서 조각 찾기
- **과정**:
  1. 사용자 질문을 벡터로 변환
  2. 벡터 DB에서 질문 벡터와 유사한 문서 벡터 검색
  3. 상위 K개의 가장 관련 있는 문서 조각 반환
- **비유**: 도서관에서 키워드로 관련 책 찾기

#### 3️⃣ Generation (생성) - 답변 만들기
- **목적**: 검색된 문서를 바탕으로 자연스러운 답변 생성
- **과정**:
  1. 검색된 문서 조각들을 프롬프트에 포함
  2. "이 문서를 바탕으로 질문에 답변해"라고 LLM에게 지시
  3. LLM이 문서 내용을 참고하여 답변 생성
- **비유**: 찾은 책의 내용을 읽고 자기 말로 요약 설명하기

### 📊 RAG vs 순수 LLM 비교

| 구분 | 순수 LLM | RAG 시스템 |
|------|----------|-----------|
| **지식 범위** | 학습 데이터에 한정 | 외부 문서 활용 가능 |
| **최신성** | 학습 시점까지만 | 문서 업데이트 시 반영 |
| **정확도** | 환각 위험 있음 | 근거 기반으로 높음 |
| **검증 가능성** | 출처 불명확 | 출처 추적 가능 |
| **비용** | 낮음 (한 번 호출) | 중간 (검색 + LLM) |
| **응답 속도** | 빠름 | 중간 (검색 시간 추가) |

---

## 🗄️ 2. 벡터 DB (Vector Database)

### 개념
벡터 DB는 **유사도 검색을 위한 특수한 데이터베이스**입니다.

일반 DB와의 차이:
- **일반 DB**: "정확히 일치"하는 데이터 찾기 (예: ID가 123인 사용자)
- **벡터 DB**: "가장 유사한" 데이터 찾기 (예: 이 질문과 비슷한 문서)

### 작동 원리

1. **데이터 저장**
   ```
   텍스트: "강아지는 귀여운 동물이다"
   벡터: [0.2, 0.8, 0.1, 0.5, ...] (보통 수백~수천 차원)
   ```

2. **유사도 계산**
   벡터 간 거리를 계산하여 유사도를 측정합니다:
   - **코사인 유사도**: 벡터 간 각도로 측정 (가장 많이 사용)
   - **유클리드 거리**: 벡터 간 직선 거리
   - **내적(Dot Product)**: 벡터 간 내적 값

3. **빠른 검색**
   - 수백만 개의 벡터 중에서도 밀리초 내에 검색
   - 특수한 인덱싱 알고리즘 사용 (ANN: Approximate Nearest Neighbor)

### 주요 벡터 DB 비교

| 이름 | 타입 | 장점 | 단점 | 추천 용도 |
|------|------|------|------|-----------|
| **FAISS** | In-memory | 매우 빠름, 무료 | 영속성 없음 | 개발/테스트 |
| **ChromaDB** | 로컬 설치 | 설치 쉬움, 영속성 | 확장성 제한 | 소규모 프로젝트 |
| **Pinecone** | 클라우드 | 관리 불필요, 확장성 | 유료, 외부 의존 | 프로덕션 |
| **Weaviate** | 클라우드/온프레미스 | 기능 풍부 | 설정 복잡 | 대규모 프로젝트 |

---

## 🧮 3. 임베딩 (Embedding)

### 개념
임베딩은 **텍스트를 숫자로 된 벡터(좌표)로 바꾸는 과정**입니다.

### 왜 필요한가?

컴퓨터는 텍스트를 직접 이해할 수 없습니다:
- ❌ "강아지"라는 단어의 의미를 알 수 없음
- ✅ [0.8, 0.2, 0.9, ...] 같은 숫자 배열은 계산 가능

### 임베딩의 마법

의미가 비슷한 단어/문장은 벡터 공간에서 가까운 위치에 배치됩니다!

```
2차원으로 단순화한 예시:

      고양이 ●
            
   강아지 ●     펫 ●
      
   
             자동차 ●
   
         
              비행기 ●
```

- "강아지", "고양이", "펫"은 서로 가까움 (동물 관련)
- "자동차", "비행기"는 따로 모여 있음 (탈것 관련)
- 동물 그룹과 탈것 그룹은 멀리 떨어져 있음

### 임베딩 모델의 역할

임베딩 모델은 대규모 텍스트 데이터로 학습되어, 단어/문장의 의미를 벡터로 표현하는 방법을 학습합니다.

**학습 방식 예시**:
- "강아지는 귀여운 ___이다" → "동물"이 들어갈 확률이 높음
- 이런 문맥을 수백만 번 학습하여 단어 간 관계를 이해

### 한국어 임베딩 모델 예시

- **ko-sroberta-multitask**: 한국어 특화, 다목적
- **ko-simcse-roberta**: 문장 유사도에 강함
- **BGE M3**: 다국어 지원, 성능 우수

---

## 🤖 4. LLM (Large Language Model)

### 개념
LLM은 **대규모 텍스트 데이터로 학습된 언어 모델**로, 자연어를 이해하고 생성할 수 있습니다.

### 주요 특징

1. **방대한 지식**: 수백 GB의 텍스트로 학습
2. **문맥 이해**: 문장의 맥락을 파악하여 적절한 답변 생성
3. **다목적**: 번역, 요약, 질의응답, 코드 생성 등 다양한 작업 수행

### 작동 원리 (간단히)

1. **Transformer 아키텍처**
   - 입력 텍스트를 토큰(단어 조각)으로 분할
   - 각 토큰 간의 관계를 계산 (Attention)
   - 다음에 올 가장 적절한 토큰 예측

2. **Attention 메커니즘**
   - 문장에서 중요한 부분에 집중
   - 예: "은행에 가서 돈을 찾았다" → "은행"과 "돈"의 관계를 이해

### 한국어 LLM 예시

**오픈소스 모델**:
- **EEVE**: 한국어 성능 우수, 다양한 크기
- **SOLAR**: 높은 성능, 효율적
- **Polyglot-Ko**: 한국어 특화

**API 기반 모델**:
- **GPT-4**: OpenAI, 가장 강력
- **Claude**: Anthropic, 안전성 강조
- **Gemini**: Google, 멀티모달 지원

### RAG에서 LLM의 역할

1. **검색된 문서 이해**: 관련 문서의 내용을 파악
2. **질문 의도 파악**: 사용자가 무엇을 원하는지 이해
3. **답변 생성**: 문서를 바탕으로 자연스러운 답변 작성
4. **불확실성 표현**: 문서에 답이 없으면 "모른다"고 답변

---

## 🛠️ 5. 개발 환경 구축

### 필수 도구

1. **Python** (3.9 이상 권장)
2. **가상환경** (conda 또는 venv)
3. **Git** (버전 관리)
4. **에디터** (VS Code, PyCharm 등)

### 주요 라이브러리

```
langchain          # RAG 파이프라인 구축
langchain-community
langgraph          # 복잡한 워크플로우
openai             # LLM API (GPT 사용 시)
sentence-transformers  # 임베딩 모델
faiss-cpu          # 벡터 DB
pypdf2             # PDF 처리
fastapi            # API 서버
dash               # 웹 UI
```

---

## 🎯 1주차 핵심 정리

### RAG 시스템의 3단계
1. **Indexing**: 문서 → 청크 → 벡터 → DB 저장
2. **Retrieval**: 질문 → 벡터 → 유사 문서 검색
3. **Generation**: 검색된 문서 + 질문 → LLM → 답변

### 핵심 구성 요소
- **벡터 DB**: 유사도 검색용 특수 데이터베이스
- **임베딩**: 텍스트를 숫자 벡터로 변환
- **LLM**: 자연어 이해 및 생성 모델

### 왜 RAG인가?
- ✅ 최신 정보 활용
- ✅ 도메인 특화 지식
- ✅ 근거 기반 답변
- ✅ 환각 감소

---

## 📝 다음 단계

1주차 이론 학습을 완료했습니다! 이제:
- ✅ RAG의 개념과 필요성 이해
- ✅ 벡터 DB, 임베딩, LLM의 역할 파악
- ✅ 전체 아키텍처 파악

**2주차 예고**: PDF 문서 처리와 청킹 실습이 시작됩니다!


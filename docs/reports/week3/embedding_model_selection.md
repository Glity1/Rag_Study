# 임베딩 모델 선정 이유 기술 문서

## 1. 개요

RAG 시스템에서 임베딩 모델은 문서와 질문을 벡터 공간에 매핑하여 의미적 유사도를 계산하는 핵심 구성 요소입니다. 본 프로젝트에서는 한국어 문서를 다루는 RAG 시스템을 구축하기 위해 여러 임베딩 모델을 비교 검토하고 최종 모델을 선정했습니다.

---

## 2. 후보 모델 조사

### 2.1 조사 대상 모델

| 모델명 | 제공자 | 언어 지원 | 벡터 차원 | 모델 크기 | 특징 |
|--------|--------|----------|----------|-----------|------|
| **all-MiniLM-L6-v2** | sentence-transformers | 다국어 (영어 중심) | 384 | ~80MB | 경량, 빠른 추론 속도 |
| **KR-SBERT-V40K-klueNLI-augSTS** | SNU NLP | 한국어 특화 | 768 | ~420MB | 한국어 성능 우수 |
| **paraphrase-multilingual-MiniLM-L12-v2** | sentence-transformers | 다국어 | 384 | ~420MB | 다국어 지원 |
| **distiluse-base-multilingual-cased** | sentence-transformers | 다국어 | 512 | ~680MB | 다국어 균형 모델 |

### 2.2 선정 기준

1. **한국어 성능**: 한국어 문서와 질문의 의미적 유사도를 정확히 계산할 수 있는가?
2. **추론 속도**: 실시간 검색에 적합한 추론 속도를 제공하는가?
3. **모델 크기**: 배포 및 메모리 사용량 측면에서 효율적인가?
4. **라이선스**: 상업적 사용 가능 여부
5. **커뮤니티 지원**: 활발한 사용 및 업데이트 여부

---

## 3. 실험 및 평가

### 3.1 실험 환경

- **하드웨어**: CPU 기반 (GPU 미사용)
- **데이터**: 한국어 PDF 문서 (그랜드코리아레저 공시자료 등)
- **평가 지표**: 
  - 임베딩 생성 속도 (청크당 평균 시간)
  - 검색 정확도 (Recall@K)
  - 메모리 사용량

### 3.2 실험 결과

#### 3.2.1 all-MiniLM-L6-v2

**장점**:
- ✅ **빠른 추론 속도**: 평균 1,000 청크당 14.2초 (CPU 기준)
- ✅ **경량 모델**: 약 80MB로 메모리 효율적
- ✅ **안정적인 성능**: 다양한 문서 유형에서 일관된 결과
- ✅ **활발한 커뮤니티**: sentence-transformers의 표준 모델

**단점**:
- ⚠️ **한국어 성능**: 영어 중심 모델로 한국어 의미 이해가 상대적으로 약함
- ⚠️ **벡터 차원**: 384차원으로 상대적으로 낮음

**실험 결과**:
```
임베딩 생성 속도: 1,000 청크당 14.2초
메모리 사용량: 최대 540MB (모델 로드 포함)
Recall@5: 0.68 ~ 0.76 (청킹 전략에 따라 다름)
```

#### 3.2.2 KR-SBERT-V40K-klueNLI-augSTS

**장점**:
- ✅ **한국어 특화**: KLUE 벤치마크에서 학습된 한국어 전용 모델
- ✅ **높은 벡터 차원**: 768차원으로 더 풍부한 의미 표현
- ✅ **한국어 성능 우수**: Week4 실험에서 한국어 질의응답 품질 향상 확인

**단점**:
- ⚠️ **느린 추론 속도**: 평균 속도가 약 20% 감소
- ⚠️ **큰 모델 크기**: 약 420MB로 메모리 사용량 증가
- ⚠️ **상대적으로 낮은 인지도**: all-MiniLM 대비 사용자 수 적음

**실험 결과**:
```
임베딩 생성 속도: 1,000 청크당 약 17초 (약 20% 느림)
메모리 사용량: 최대 800MB (모델 로드 포함)
Recall@5: 0.72 ~ 0.80 (한국어 질문에서 더 높은 성능)
```

---

## 4. 최종 선정: all-MiniLM-L6-v2

### 4.1 선정 이유

1. **속도와 성능의 균형**: 
   - 한국어 성능이 약간 낮더라도 실시간 검색에 적합한 속도 제공
   - 프로젝트 초기 단계에서 빠른 프로토타이핑 가능

2. **안정성과 호환성**:
   - sentence-transformers의 표준 모델로 안정적
   - LangChain과의 호환성 우수
   - 다양한 환경에서 검증된 모델

3. **리소스 효율성**:
   - 경량 모델로 배포 및 확장 용이
   - CPU 환경에서도 충분한 성능 제공

4. **유연성**:
   - 필요 시 KR-SBERT 등 다른 모델로 쉽게 교체 가능
   - 모델별 인덱스를 분리하여 A/B 테스트 가능

### 4.2 Trade-off 분석

| 항목 | all-MiniLM-L6-v2 | KR-SBERT | 선택 이유 |
|------|-----------------|----------|----------|
| **한국어 성능** | 보통 | 우수 | 실시간 검색에서는 속도가 더 중요 |
| **추론 속도** | 빠름 | 보통 | 사용자 경험 측면에서 속도 우선 |
| **메모리 사용** | 낮음 | 높음 | 배포 및 확장성 고려 |
| **안정성** | 높음 | 보통 | 프로덕션 환경에서 안정성 중요 |

---

## 5. 향후 개선 방향

### 5.1 단기 개선

1. **모델별 인덱스 분리**:
   - 각 모델별로 별도 인덱스 디렉터리 사용
   - A/B 테스트를 통한 성능 비교

2. **하이브리드 접근**:
   - 빠른 검색: all-MiniLM 사용
   - 정밀 검색: KR-SBERT 사용
   - 사용자 요구에 따라 선택

### 5.2 장기 개선

1. **Fine-tuning**:
   - 도메인 특화 데이터로 all-MiniLM fine-tuning
   - 한국어 성능 향상과 속도 유지

2. **최신 모델 검토**:
   - 새로운 한국어 임베딩 모델 출현 시 재평가
   - 예: `jhgan/ko-sroberta-multitask`, `BM-K/KoSimCSE-roberta-multitask`

3. **양자화 및 최적화**:
   - FAISS PQ (Product Quantization) 적용
   - 대규모 인덱스 처리 최적화

---

## 6. 참고 자료

- [sentence-transformers Documentation](https://www.sbert.net/)
- [SNU NLP KR-SBERT](https://github.com/snunlp/KR-SBERT)
- [KLUE Benchmark](https://klue-benchmark.com/)
- [FAISS Documentation](https://github.com/facebookresearch/faiss)

---

## 7. 결론

본 프로젝트에서는 **all-MiniLM-L6-v2**를 기본 임베딩 모델로 선정했습니다. 이는 한국어 성능이 약간 낮더라도 실시간 검색에 적합한 속도, 안정성, 리소스 효율성을 제공하기 때문입니다. 

다만, 한국어 성능이 중요한 경우에는 **KR-SBERT-V40K-klueNLI-augSTS**를 대안으로 사용할 수 있으며, 프로젝트 구조상 모델 교체가 용이하도록 설계되었습니다.

**선정일**: 2024년 11월  
**검토 주기**: 분기별 (새로운 모델 출현 시 재평가)

